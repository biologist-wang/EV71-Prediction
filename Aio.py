import pandas as pd
import numpy as np
import os
from scipy.stats import mannwhitneyu, chi2_contingency
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
import shap
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \
    ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
import joblib


# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆæ ¹æ®ç³»ç»Ÿç¯å¢ƒå¯èƒ½éœ€è¦è°ƒæ•´ï¼Œå¦‚ Windows ç”¨ SimHeiï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# æ ¸å¿ƒé…ç½®ï¼šç†åŒ–æ€§è´¨ Z-scales (Sandberg et al.)
# P1(P2124)->Z1, P2(P997)->Z2, P3(P1246)->Z3, P4(P1743)->Z2, P5(P1711)->Z1
# ==========================================
z_dict = {
    'A': [0.07, -1.73, 0.09], 'R': [2.88, 2.52, -3.44], 'N': [3.22, 1.45, 0.84],
    'D': [3.64, 1.13, 2.36], 'C': [0.71, -0.97, 4.15], 'Q': [2.18, 0.53, -1.14],
    'E': [3.08, 0.39, -0.07], 'G': [2.23, -5.36, 0.30], 'H': [2.41, 1.74, 1.11],
    'I': [-4.44, -1.68, -1.03], 'L': [-4.19, -1.03, -0.98], 'K': [2.84, 1.41, -3.14],
    'M': [-2.49, -0.27, -0.41], 'F': [-4.92, 1.30, 0.45], 'P': [-1.22, 0.88, 2.23],
    'S': [1.96, -1.63, 0.57], 'T': [0.92, -2.09, -1.40], 'W': [-4.75, 3.65, 0.85],
    'Y': [-1.39, 2.32, 0.01], 'V': [-2.69, -2.53, -1.29], '-': [0, 0, 0]
}

# ==========================================
# æ•°æ®æ¸…æ´—ä¸å¹´ä»½ä¿®æ­£
# ==========================================
def step1_prepare_master(file_path):
    df = pd.read_csv(file_path)

    # A. ä¸´åºŠæ ‡ç­¾åˆ¤å®š
    cns_keys = ['CSF', 'BRAIN', 'CNS', 'ENCEPHALITIS', 'SPINAL', 'NERVE']
    df['Label'] = df['Tissue_Specimen_Source'].fillna('Unknown').str.upper().apply(
        lambda x: 1 if any(k in x for k in cns_keys) else 0
    )

    df['Year'] = pd.to_numeric(df['Collection_Date'], errors='coerce')
    if not df['Year'].isnull().all():
        df['Year'] = df['Year'].fillna(df['Year'].mode()[0]).astype(int)
    else:
        df['Year'] = 2012  # æœ€åçš„å¼ºåˆ¶ä¿åº•

    # C. åœ°ç†ä½ç½®æå–
    df['Country'] = df['Geo_Location'].fillna('Unknown').str.split(':').str[0].str.strip()

    # D. æ°¨åŸºé…¸ Z-scales ç¼–ç 
    std_len = df['aa'].apply(len).mode()[0]
    encoded = []
    for seq in df['aa']:
        vec = []
        for i in range(std_len):
            char = seq[i] if i < len(seq) else '-'
            vec.extend(z_dict.get(char, [0, 0, 0]))
        encoded.append(vec)

    feat_names = [f"P{i + 1}_Z{j}" for i in range(std_len) for j in [1, 2, 3]]
    X_matrix = pd.DataFrame(encoded, columns=feat_names)

    master_df = pd.concat([df[['Accession', 'Country', 'Year', 'Label']], X_matrix], axis=1)
    master_df.to_csv('EV71_Master_Dataset.csv', index=False, encoding='utf_8_sig')
    print(f"âœ… Master æ•°æ®å›ºåŒ–å®Œæˆã€‚CNS æ ·æœ¬: {sum(df['Label'] == 1)}, Non-CNS: {sum(df['Label'] == 0)}")
    return master_df

# ==========================================
# æ ¸å¿ƒç»Ÿè®¡åˆ†æä¸ Table 1 ç”Ÿæˆï¼ˆåŸºçº¿è¡¨ï¼‰
# ==========================================
def step2_generate_clinical_table(df, top_n=25):
    pos_df = df[df['Label'] == 1]
    neg_df = df[df['Label'] == 0]
    rows = []

    # --- 1. åœ°ç†åˆ†å¸ƒ (å¡æ–¹æ£€éªŒ) ---
    ctab = pd.crosstab(df['Country'], df['Label'])
    _, p_country, _, _ = chi2_contingency(ctab)
    rows.append({
        'ç‰¹å¾å˜é‡': 'åœ°ç†åˆ†å¸ƒ (Country)', 'ç»Ÿè®¡æè¿°æ–¹æ³•': 'n (%)', 'ç»Ÿè®¡æ£€éªŒæ–¹æ³•': 'å¡æ–¹æ£€éªŒ',
        'CNSç»„ (n=7)': '-', 'Non-CNSç»„ (n=260)': '-', 'På€¼': f"{p_country:.4f}", 'OR (95% CI)': '-'
    })
    for c in sorted(df['Country'].unique()):
        n_p = sum(pos_df['Country'] == c)
        n_n = sum(neg_df['Country'] == c)
        rows.append({
            'ç‰¹å¾å˜é‡': f"  - {c}", 'ç»Ÿè®¡æè¿°æ–¹æ³•': 'n (%)', 'ç»Ÿè®¡æ£€éªŒæ–¹æ³•': '-',
            'CNSç»„ (n=7)': f"{n_p} ({n_p / 7:.1%})",
            'Non-CNSç»„ (n=260)': f"{n_n} ({n_n / 260:.1%})",
            'På€¼': '', 'OR (95% CI)': '-'
        })

    # --- 2. é‡‡é›†å¹´ä»½ (Median + IQR) ---
    _, p_year = mannwhitneyu(pos_df['Year'], neg_df['Year'])
    rows.append({
        'ç‰¹å¾å˜é‡': 'é‡‡é›†å¹´ä»½', 'ç»Ÿè®¡æè¿°æ–¹æ³•': 'Median (IQR)', 'ç»Ÿè®¡æ£€éªŒæ–¹æ³•': 'Mann-Whitney U',
        'CNSç»„ (n=7)': f"{pos_df['Year'].median():.0f} ({pos_df['Year'].quantile(0.25):.0f}-{pos_df['Year'].quantile(0.75):.0f})",
        'Non-CNSç»„ (n=260)': f"{neg_df['Year'].median():.0f} ({neg_df['Year'].quantile(0.25):.0f}-{neg_df['Year'].quantile(0.75):.0f})",
        'På€¼': f"{p_year:.4f}", 'OR (95% CI)': '-'
    })

    # --- 3. ç†åŒ–ä½ç‚¹åˆ†æ ---
    phys_cols = [c for c in df.columns if c.startswith('P')]
    p_results = []
    for col in phys_cols:
        _, p = mannwhitneyu(pos_df[col], neg_df[col])
        p_results.append({'feat': col, 'p': p})

    # ç­›é€‰æœ€æ˜¾è‘—çš„ Top ä½ç‚¹
    top_feats = pd.DataFrame(p_results).sort_values('p').head(top_n)
    z_map = {'Z1': 'ç–æ°´æ€§', 'Z2': 'åˆ†å­é‡/ä½“ç§¯', 'Z3': 'ææ€§/ç”µè·'}

    for _, f_info in top_feats.iterrows():
        col = f_info['feat']
        p_val = f_info['p']

        # æè¿°ç»Ÿè®¡ï¼šç»Ÿä¸€ Median (IQR)
        q1_p, q3_p = pos_df[col].quantile(0.25), pos_df[col].quantile(0.75)
        q1_n, q3_n = neg_df[col].quantile(0.25), neg_df[col].quantile(0.75)
        desc_pos = f"{pos_df[col].median():.2f} ({q1_p:.2f}-{q3_p:.2f})"
        desc_neg = f"{neg_df[col].median():.2f} ({q1_n:.2f}-{q3_n:.2f})"

        # è®¡ç®— OR (95% CI)
        try:
            X = sm.add_constant(df[col])
            logit_mod = sm.Logit(df['Label'], X).fit(disp=0)
            or_val = np.exp(logit_mod.params[1])
            conf = np.exp(logit_mod.conf_int().iloc[1])
            or_str = f"{or_val:.2f} ({conf[0]:.2f}-{conf[1]:.2f})"
        except:
            or_str = "N/A (æ ·æœ¬æåº¦ä¸å¹³è¡¡)"

        z_type = col.split('_')[-1]
        rows.append({
            'ç‰¹å¾å˜é‡': f"{col} ({z_map[z_type]})",
            'ç»Ÿè®¡æè¿°æ–¹æ³•': 'Median (IQR)',
            'ç»Ÿè®¡æ£€éªŒæ–¹æ³•': 'Mann-Whitney U',
            'CNSç»„ (n=7)': desc_pos,
            'Non-CNSç»„ (n=260)': desc_neg,
            'På€¼': f"{p_val:.4e}",
            'OR (95% CI)': or_str
        })

    # è¾“å‡ºæ–‡ä»¶
    final_df = pd.DataFrame(rows)
    final_df.to_csv('Clinical_Adaptive_Table1.csv', index=False, encoding='utf_8_sig')
    print(f"âœ¨ æœ€ç»ˆç»Ÿè®¡è¡¨å·²ç”Ÿæˆ: Clinical_Adaptive_Table1.csv")
    return top_feats['feat'].tolist()

# ==========================================
# ä½ç‚¹å·®å¼‚å›¾ç”Ÿæˆ
# ==========================================
def generate_forest_plot(
        input_table='Clinical_Adaptive_Table1.csv',
        top_n_plot=15,
        output_pdf='Neurovirulence_Forest_Plot.pdf'
):
    """ä»…è´Ÿè´£æ£®æ—å›¾çš„è§£æä¸ç»˜åˆ¶"""
    print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆæ£®æ—å›¾: {output_pdf}...")
    table1 = pd.read_csv(input_table)

    # 1. ç­›é€‰å¹¶è§£æ OR å€¼
    plot_df = table1[table1['OR (95% CI)'].str.contains(r'\(', na=False)].copy()

    def parse_or(x):
        try:
            parts = x.split(' ')
            val = float(parts[0])
            ci_parts = parts[1].strip('()').split('-')
            return val, float(ci_parts[0]), float(ci_parts[1])
        except:
            return 1.0, 1.0, 1.0

    plot_df[['OR', 'Lower', 'Upper']] = plot_df['OR (95% CI)'].apply(lambda x: pd.Series(parse_or(x)))
    plot_df = plot_df.sort_values('På€¼').head(top_n_plot)

    # 2. ç»˜å›¾
    plt.figure(figsize=(10, 8))
    y_pos = np.arange(len(plot_df))

    # ç»˜åˆ¶ CI çº¿ä¸ OR ç‚¹
    plt.errorbar(plot_df['OR'], y_pos,
                 xerr=[plot_df['OR'] - plot_df['Lower'], plot_df['Upper'] - plot_df['OR']],
                 fmt='s', color='firebrick', ecolor='steelblue', capsize=4,
                 markersize=8, label='Odds Ratio (95% CI)')

    plt.axvline(x=1, color='black', linestyle='--', alpha=0.7)
    plt.yticks(y_pos, plot_df['ç‰¹å¾å˜é‡'])
    plt.xlabel('æ¯”å€¼æ¯” (Odds Ratio) ä¸ 95% ç½®ä¿¡åŒºé—´')
    plt.title(f'EV71 ç¥ç»æ¯’åŠ›å…³è”ä½ç‚¹æ£®æ—å›¾ (Top {top_n_plot})')
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()

    plt.savefig(output_pdf, bbox_inches='tight')
    plt.close()
    print(f"âœ… æ£®æ—å›¾å·²å¯¼å‡ºã€‚")

# ==========================================
# æ¨¡å‹è®­ç»ƒå¹¶å¯¼å‡º
# ==========================================
def train_and_freeze_model(
        input_table='Clinical_Adaptive_Table1.csv',
        master_csv='Master_Dataset.csv',
        top_n_model=5,
        model_output_name='Combined_Predictive_Model.pkl',
        roc_pdf='Prediction_ROC_Curve.pdf'
):
    """è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯åŠæŒä¹…åŒ–å›ºåŒ–"""

    table1 = pd.read_csv(input_table)
    master_df = pd.read_csv(master_csv)

    # 1. åŠ¨æ€ç­›é€‰ç‰¹å¾ (ä»ç»Ÿè®¡è¡¨ P å€¼æœ€å°çš„ç†åŒ–ä½ç‚¹ä¸­é€‰å–)
    # ç¡®ä¿åªé€‰å–ä»¥ 'P' å¼€å¤´çš„ç†åŒ–ç‰¹å¾åˆ—
    physico_feats = table1[table1['ç‰¹å¾å˜é‡'].str.contains('^P', regex=True)]
    top_cols = physico_feats.sort_values('På€¼')['ç‰¹å¾å˜é‡'].str.split(' ').str[0].tolist()[:top_n_model]

    X = master_df[top_cols]
    y = master_df['Label']

    # 2. è®­ç»ƒæ¨¡å‹
    lr_model = LogisticRegression(class_weight='balanced', solver='liblinear')
    lr_model.fit(X, y)

    # 3. å›ºåŒ–æ¨¡å‹åŠå…¶å…ƒæ•°æ® (åŒ…å«ç‰¹å¾é¡ºåº)
    model_payload = {
        'model': lr_model,
        'features': top_cols,
        'model_type': 'Logistic Regression',
        'n_samples': len(master_df)
    }
    joblib.dump(model_payload, model_output_name)
    print(f"ğŸ’¾ æ¨¡å‹åŠç‰¹å¾å…ƒæ•°æ®å·²å›ºåŒ–è‡³: {model_output_name}")

    # 4. ç”Ÿæˆ ROC è¯„ä¼°å›¾
    y_scores = lr_model.predict_proba(X)[:, 1]
    fpr, tpr, _ = roc_curve(y, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(7, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
    plt.xlabel('False Positive Rate');
    plt.ylabel('True Positive Rate')
    plt.title(f'Top {top_n_model} ç‰¹å¾æ¨¡å‹é¢„æµ‹æ€§èƒ½')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.2)
    plt.savefig(roc_pdf, bbox_inches='tight')
    plt.close()

    print(f"âœ… ROC æ›²çº¿å·²å¯¼å‡º: {roc_pdf} (AUC: {roc_auc:.3f})")
    return top_cols



def extract_model_formula_and_predict():

    # 1. åŠ è½½æ•°æ®
    master_df = pd.read_csv('Master_Dataset.csv')
    table1 = pd.read_csv('Clinical_Adaptive_Table1.csv')

    # 2. è·å– Top 5 æ ¸å¿ƒç‰¹å¾ (åŸºäº P å€¼æ’å)
    # è¿‡æ»¤æ‰éç†åŒ–ç‰¹å¾è¡Œï¼Œåªå– P å¼€å¤´çš„ç‰¹å¾
    physico_results = table1[table1['ç‰¹å¾å˜é‡'].str.startswith('P', na=False)]
    top_5_features = physico_results.sort_values('På€¼')['ç‰¹å¾å˜é‡'].str.split(' ').str[0].tolist()[:5]

    print(f"âœ… é€‰å®šçš„æ ¸å¿ƒå»ºæ¨¡æŒ‡æ ‡: {top_5_features}")

    # 3. æ„å»ºå¤šå› ç´ é€»è¾‘å›å½’æ¨¡å‹ (ä½¿ç”¨ statsmodels ä»¥è·å–è¯¦ç»†ç»Ÿè®¡å‚æ•°)
    X = master_df[top_5_features]
    y = master_df['Label']
    X_with_const = sm.add_constant(X)  # æ·»åŠ å¸¸æ•°é¡¹ (Intercept)

    model = sm.Logit(y, X_with_const).fit(disp=0)

    # 4. å¯¼å‡ºæƒé‡ç³»æ•°è¡¨
    summary_df = pd.DataFrame({
        'ç‰¹å¾ä½ç‚¹': X_with_const.columns,
        'æƒé‡ç³»æ•° (Beta)': model.params,
        'På€¼': model.pvalues,
        'ORå€¼': np.exp(model.params)
    })
    summary_df.to_csv('Model_Coefficients_Weight.csv', index=False, encoding='utf_8_sig')

    # 5. è®¡ç®—å…¨æ ·æœ¬é¢„æµ‹æ¦‚ç‡
    # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç»™æ¯ä¸ªæ ·æœ¬æ‰“åˆ†
    master_df['Risk_Probability'] = model.predict(X_with_const)

    # 6. ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
    # é‡ç‚¹çœ‹ CNS ç»„çš„é¢„æµ‹è¡¨ç°
    report_df = master_df[['Accession', 'Label', 'Year', 'Country', 'Risk_Probability']].copy()
    report_df = report_df.sort_values('Risk_Probability', ascending=False)

    # ä¿å­˜é¢„æµ‹æ¸…å•
    report_df.to_csv('Sample_Risk_Predictions.csv', index=False, encoding='utf_8_sig')

    print("-" * 30)
    print("ğŸ“ˆ æ¨¡å‹æ•°å­¦å…¬å¼é¢„è§ˆ (Logit P = Î£ Beta*X + Const):")
    for i, row in summary_df.iterrows():
        print(f"   [{row['ç‰¹å¾ä½ç‚¹']}] æƒé‡: {row['æƒé‡ç³»æ•° (Beta)']:+.4f}")

    print("-" * 30)
    # æ£€æŸ¥é‚£ 7 ä¸ª CNS æ ·æœ¬çš„å¹³å‡é¢„æµ‹å¾—åˆ†
    cns_avg = report_df[report_df['Label'] == 1]['Risk_Probability'].mean()
    non_cns_avg = report_df[report_df['Label'] == 0]['Risk_Probability'].mean()
    print(f"ğŸ“Š CNSç»„å¹³å‡é£é™©å¾—åˆ†: {cns_avg:.2%}")
    print(f"ğŸ“Š Non-CNSç»„å¹³å‡é£é™©å¾—åˆ†: {non_cns_avg:.2%}")
    print("âœ¨ é¢„æµ‹æ¸…å•å·²ä¿å­˜è‡³: Sample_Risk_Predictions.csv")


# ==========================================
# æ ¸å¿ƒä½ç‚¹ç†åŒ–æ™¯è§‚çƒ­å›¾
# ==========================================
def plot_physicochemical_heatmap():

    # 1. åŠ è½½æ•°æ®
    df = pd.read_csv('Master_Dataset.csv')
    top_features = ['P2124_Z1', 'P1246_Z3', 'P997_Z2', 'P1711_Z1', 'P1743_Z2']

    # 2. å‡†å¤‡ç»˜å›¾æ•°æ®ï¼šæå– 7 ä¸ª CNS æ ·æœ¬å’Œ éšæœº 20 ä¸ª Non-CNS æ ·æœ¬è¿›è¡Œå¯¹æ¯”
    cns_samples = df[df['Label'] == 1]
    non_cns_samples = df[df['Label'] == 0].sample(20, random_state=42)
    plot_data = pd.concat([cns_samples, non_cns_samples])

    # 3. æ•°æ®æ ‡å‡†åŒ– (Z-score å½’ä¸€åŒ–ï¼Œä½¿ä¸åŒæ€§è´¨çš„ Z-scales å…·æœ‰å¯æ¯”æ€§)
    scaler = StandardScaler()
    scaled_values = scaler.fit_transform(plot_data[top_features])
    plot_df_scaled = pd.DataFrame(scaled_values, columns=top_features)
    plot_df_scaled['Group'] = ['CNS'] * len(cns_samples) + ['Non-CNS'] * len(non_cns_samples)

    # 4. ç»˜å›¾
    plt.figure(figsize=(12, 8))
    # è®¾ç½®ä¾§è¾¹é¢œè‰²æ¡ï¼ŒåŒºåˆ†ç»„åˆ«
    group_colors = plot_df_scaled['Group'].map({'CNS': 'firebrick', 'Non-CNS': 'dodgerblue'})

    g = sns.clustermap(
        plot_df_scaled[top_features],
        cmap='RdYlBu_r',  # çº¢è“è‰²è°ƒï¼Œçº¢è‰²ä»£è¡¨ç†åŒ–åˆ†å€¼é«˜ï¼Œè“è‰²ä»£è¡¨ä½
        row_colors=group_colors,
        yticklabels=False,
        linewidths=.5,
        cbar_pos=(0.02, 0.8, 0.03, 0.15)
    )

    plt.title('EV71 ç¥ç»æ¯’åŠ›æ ¸å¿ƒä½ç‚¹ç†åŒ–æ™¯è§‚çƒ­å›¾', pad=100)
    g.savefig('Physicochemical_Heatmap.pdf', dpi=300, bbox_inches='tight')
    print("âœ… çƒ­å›¾å·²ä¿å­˜ï¼šPhysicochemical_Heatmap")

# ==========================================
# ç•™ä¸€æ³•äº¤å‰éªŒè¯éªŒè¯
# ==========================================
def run_loocv_validation():

    # 1. åŠ è½½æ•°æ®
    # ç¡®ä¿ä¹‹å‰å·²ç»è¿è¡Œè¿‡ step1 ç”Ÿæˆäº† master æ•°æ®é›†
    df = pd.read_csv('Master_Dataset.csv')

    # 2. å®šä¹‰å»ºæ¨¡ç‰¹å¾ (ä½¿ç”¨ä½ ä¹‹å‰é€‰å®šçš„ Top 5)
    features = ['P2124_Z1', 'P1246_Z3', 'P997_Z2', 'P1711_Z1', 'P1743_Z2']
    X = df[features].values
    y = df['Label'].values

    # 3. åˆå§‹åŒ–éªŒè¯ç¯å¢ƒ
    loo = LeaveOneOut()
    y_true = []
    y_probs = []

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨ class_weight='balanced' åº”å¯¹æ ·æœ¬ä¸å¹³è¡¡ï¼‰
    lr = LogisticRegression(class_weight='balanced', solver='liblinear')

    # 4. æ‰§è¡Œäº¤å‰éªŒè¯å¾ªç¯
    # æ¯æ¬¡ç•™å‡ºä¸€ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•ï¼Œå…¶ä½™å»ºæ¨¡
    count = 0
    for train_index, test_index in loo.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # è®­ç»ƒæ¨¡å‹
        lr.fit(X_train, y_train)

        # é¢„æµ‹è¢«ç•™å‡ºçš„é‚£ä¸ªæ ·æœ¬å±äº CNS çš„æ¦‚ç‡
        prob = lr.predict_proba(X_test)[:, 1]

        y_true.append(y_test[0])
        y_probs.append(prob[0])

        count += 1
        if count % 50 == 0:
            print(f"å·²å®Œæˆ {count}/267 ä¸ªæ ·æœ¬çš„è½®è½¬éªŒè¯...")

    # 5. è®¡ç®—éªŒè¯åçš„ ROC å’Œ AUC
    fpr, tpr, _ = roc_curve(y_true, y_probs)
    cv_auc = auc(fpr, tpr)

    # 6. ç»˜å›¾ï¼šéªŒè¯åçš„ ROC æ›²çº¿
    plt.figure(figsize=(8, 7))
    plt.plot(fpr, tpr, color='darkgreen', lw=2, label=f'LOOCV ROC (AUC = {cv_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (å‡é˜³æ€§ç‡)')
    plt.ylabel('True Positive Rate (çœŸé˜³æ€§ç‡)')
    plt.title('ç•™ä¸€äº¤å‰éªŒè¯ (LOOCV) ROC æ›²çº¿\n(è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›)')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)

    plt.savefig('LOOCV_Validation_ROC.pdf', dpi=300, bbox_inches='tight')
    print(f"\nâœ… éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“Š äº¤å‰éªŒè¯åçš„ AUC ä¸º: {cv_auc:.3f}")

    # 7. ä¿å­˜éªŒè¯åçš„é¢„æµ‹æ‰“åˆ†ï¼Œç”¨äºåˆ†æå“ªäº›æ ·æœ¬è¢«â€œç®—é”™â€äº†
    val_results = pd.DataFrame({
        'Accession': df['Accession'],
        'Actual_Label': y_true,
        'Predicted_Prob': y_probs
    })
    val_results.to_csv('LOOCV_Prediction_Results.csv', index=False)

# ==========================================
# æ¨¡å‹å¯¹æ¯”å¹¶ä¿å­˜è¡¨ç°æœ€å¥½æ¨¡å‹
# ==========================================
def compare_eight_models_and_save_best(
        master_csv='Master_Dataset.csv',
        model_output_path='./model/Best_Model_Package.pkl'
):


    # 1. æ•°æ®å‡†å¤‡
    df = pd.read_csv(master_csv)
    features = ['P2124_Z1', 'P1246_Z3', 'P997_Z2', 'P1711_Z1', 'P1743_Z2']

    # æ ¸å¿ƒï¼šå¿…é¡»ä¿å­˜è¿™ä¸ª scalerï¼Œå¦åˆ™æ¨¡å‹æ— æ³•åœ¨å…¶ä»–æ•°æ®é›†ä¸Šä½¿ç”¨
    scaler = StandardScaler()
    X = scaler.fit_transform(df[features].values)
    y = df['Label'].values

    # 2. å®šä¹‰ 8 ç§æ¨¡å‹
    models = {
        'Logistic Regression': LogisticRegression(class_weight='balanced', solver='liblinear'),
        'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
        'SVM (RBF)': SVC(probability=True, class_weight='balanced', kernel='rbf'),
        'Extra Trees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
        'Ridge Classifier': RidgeClassifier(class_weight='balanced'),
        'Gaussian Naive Bayes': GaussianNB()
    }

    plt.figure(figsize=(12, 9))
    loo = LeaveOneOut()
    results = []

    best_auc = 0
    best_model_name = ""

    # 3. æ ¸å¿ƒå¾ªç¯ï¼šLOOCV éªŒè¯
    for name, model in models.items():
        print(f"æ­£åœ¨æµ‹è¯•ç®—æ³•: {name}...")
        y_true, y_probs = [], []

        for train_index, test_index in loo.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            model.fit(X_train, y_train)

            if name == 'Ridge Classifier':
                d = model.decision_function(X_test)
                prob = 1 / (1 + np.exp(-d))
            else:
                prob = model.predict_proba(X_test)[:, 1]

            y_true.append(y_test[0])
            y_probs.append(prob[0])

        # è®¡ç®—å¹¶ç»˜å›¾
        fpr, tpr, _ = roc_curve(y_true, y_probs)
        roc_auc = auc(fpr, tpr)
        results.append({'Model': name, 'AUC': roc_auc})
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

        # è®°å½•è¡¨ç°æœ€å¥½çš„æ¨¡å‹
        if roc_auc > best_auc:
            best_auc = roc_auc
            best_model_name = name

    # 4. å›¾è¡¨ä¿®é¥°
    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
    plt.xlabel('False Positive Rate');
    plt.ylabel('True Positive Rate')
    plt.title('8 ç§æ¨¡å‹é¢„æµ‹æ€§èƒ½å¯¹æ¯” (LOOCV)')
    plt.legend(loc="lower right", fontsize='small', ncol=2)
    plt.savefig('Eight_Models_Comparison_ROC.pdf', dpi=300, bbox_inches='tight')
    plt.close()

    # 5. é‡æ–°è®­ç»ƒå† å†›æ¨¡å‹å¹¶å›ºåŒ–å¯¼å‡º
    print(f"\nğŸ† å† å†›æ¨¡å‹ç¡®è®¤: {best_model_name} (AUC: {best_auc:.4f})")

    # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒå† å†›ç®—æ³•
    final_best_model = models[best_model_name]
    final_best_model.fit(X, y)

    # å¯¼å‡ºâ€œé¢„æµ‹å…¨å®¶æ¡¶â€
    best_package = {
        'model_name': best_model_name,
        'model': final_best_model,
        'scaler': scaler,  # é¢„æµ‹æ–°åºåˆ—æ—¶å¿…é¡»å…ˆç”¨å®ƒç¼©æ”¾æ•°æ®
        'features': features,  # è®°å½•ç‰¹å¾é¡ºåº
        'auc_score': best_auc
    }
    joblib.dump(best_package, model_output_path)

    # è¾“å‡ºæ’åæŠ¥å‘Š
    report = pd.DataFrame(results).sort_values('AUC', ascending=False)
    report.to_csv('Eight_Models_Ranking.csv', index=False)

    print(f"ğŸ’¾ å† å†›æ¨¡å‹å…¨å®¶æ¡¶å·²å¯¼å‡ºè‡³: {model_output_path}")
    return report

# ==========================================
# shap
# ==========================================
def generate_shap_analysis():
    # 1. åŠ è½½æ•°æ®
    df = pd.read_csv('Master_Dataset.csv')
    features = ['P2124_Z1', 'P1246_Z3', 'P997_Z2', 'P1711_Z1', 'P1743_Z2']
    X = df[features]
    y = df['Label']

    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=features)

    # 2. é‡æ–°è®­ç»ƒä½ çš„å† å†›æ¨¡å‹ (Logistic Regression)
    model = LogisticRegression(class_weight='balanced', solver='liblinear')
    model.fit(X_scaled, y)

    # 3. åˆ›å»º SHAP è§£é‡Šå™¨
    # å¯¹äºé€»è¾‘å›å½’ï¼Œæˆ‘ä»¬ä½¿ç”¨ LinearExplainer
    explainer = shap.LinearExplainer(model, X_scaled)
    shap_values = explainer.shap_values(X_scaled)

    # 4. ç»˜åˆ¶ SHAP Summary Plot (æ¡å½¢å›¾ï¼šå±•ç¤ºæ•´ä½“ç‰¹å¾é‡è¦æ€§)
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_scaled, plot_type="bar", show=False)
    plt.title('æ ¸å¿ƒä½ç‚¹å¯¹ç¥ç»æ¯’åŠ›é¢„æµ‹çš„è´¡çŒ®åº¦æ’å (SHAP Importance)')
    plt.tight_layout()
    plt.savefig('SHAP_Feature_Importance.pdf', dpi=300, bbox_inches='tight')

    # 5. ç»˜åˆ¶ SHAP Summary Plot (æ•£ç‚¹å›¾ï¼šå±•ç¤ºç‰¹å¾å–å€¼é«˜ä½å¯¹é£é™©çš„å½±å“)
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_scaled, show=False)
    plt.title('ä½ç‚¹ç†åŒ–å€¼åˆ†å¸ƒå¯¹é£é™©çš„å½±å“åˆ†æ (SHAP Summary)')
    plt.tight_layout()
    plt.savefig('SHAP_Summary_Distribution.pdf', dpi=300, bbox_inches='tight')

# ==========================================
# åˆ—çº¿å›¾
# ==========================================
def generate_nomogram_data():
    # 1. å®šä¹‰æ¨¡å‹å‚æ•°
    intercept = -7.5129
    weights = {
        'P2124_Z1': 1.1684,
        'P997_Z2': 0.9244,
        'P1743_Z2': 0.2431,
        'P1711_Z1': 0.0728,
        'P1246_Z3': -0.2441
    }

    # è®¾ç½®ä¸­æ–‡æ”¯æŒä¸å…¨å±€å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

    # è®¡ç®—æœ€å¤§æ³¢åŠ¨èŒƒå›´ä»¥å½’ä¸€åŒ– Points
    max_impact = max([abs(w) * 6 for w in weights.values()])

    # å¢åŠ ç”»å¸ƒé«˜åº¦ï¼Œé¿å…çºµå‘æ‹¥æŒ¤
    fig, ax = plt.subplots(figsize=(14, 10))

    # ---------------------------------------------------------
    # 2. ç»˜åˆ¶é¡¶éƒ¨ Point è½´ (å‚è€ƒåŸºå‡†)
    # ---------------------------------------------------------
    y_base = 10
    ax.hlines(y_base, 0, 100, colors='black', lw=1.5)
    # å¾®è°ƒç‚¹ï¼šä½¿ç”¨ bbox å¢åŠ æ ‡ç­¾å¯è¯»æ€§ï¼Œè°ƒæ•´æ¨ªå‘åç§»
    ax.text(-2, y_base, 'å•é¡¹è¯„åˆ† (Points)', fontweight='bold', ha='right', va='center', fontsize=12)

    for x in range(0, 101, 10):
        ax.vlines(x, y_base, y_base + 0.2, colors='black')
        ax.text(x, y_base + 0.4, str(x), ha='center', fontsize=10)

    # ---------------------------------------------------------
    # 3. ç»˜åˆ¶å„ä¸ªç‰¹å¾è½´
    # ---------------------------------------------------------
    y_pos = 8.5  # èµ·å§‹ä½ç½®
    for feat, weight in sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True):
        ax.hlines(y_pos, 0, 100, colors='lightgray', linestyle='--', alpha=0.6)

        ax.text(-2, y_pos, f"{feat}", fontweight='bold', ha='right', va='center')

        points_per_unit = (abs(weight) * 6) / max_impact * 100

        # æ ‡æ³¨åˆ»åº¦
        ticks = [-3, -2, -1, 0, 1, 2, 3]
        for val in ticks:
            # é€»è¾‘å¾®è°ƒï¼šå¦‚æœæƒé‡ä¸ºè´Ÿï¼Œåˆ»åº¦å€¼ä»å°åˆ°å¤§åº”å¯¹åº”åˆ†å€¼ä»å¤§åˆ°å°
            if weight > 0:
                p = (val + 3) / 6 * points_per_unit
            else:
                p = (3 - val) / 6 * points_per_unit

            ax.vlines(p, y_pos, y_pos + 0.15, colors='navy', lw=1)
            # ä»…åœ¨æ•´æ•°ç‚¹æ ‡æ³¨æ–‡å­—ï¼Œé¿å…æ‹¥æŒ¤
            ax.text(p, y_pos - 0.4, f"{val}", fontsize=9, ha='center', color='#333333')

        y_pos -= 1.4  # å¢åŠ è¡Œé—´è·

    # ---------------------------------------------------------
    # 4. åº•éƒ¨æ€»åˆ†ä¸æ¦‚ç‡è½´
    # ---------------------------------------------------------
    y_total = 0
    ax.hlines(y_total, 0, 100, colors='black', lw=2)
    ax.text(-2, y_total, 'æ€»è¯„åˆ† (Total Points)', fontweight='bold', ha='right', color='black', fontsize=12)
    for x in range(0, 101, 20):
        ax.vlines(x, y_total, y_total + 0.3, colors='black')
        ax.text(x, y_total - 0.5, str(x * 4), ha='center')  # å‡è®¾æ€»åˆ†ä¸ºå•é¡¹åˆ†ç´¯åŠ æ˜ å°„

    y_prob = -2
    ax.hlines(y_prob, 0, 100, colors='darkred', lw=2)
    ax.text(-2, y_prob, 'ç¥ç»æ¯’åŠ›é£é™©æ¦‚ç‡', fontweight='bold', ha='right', color='darkred', fontsize=12)

    # æ¦‚ç‡åˆ»åº¦éçº¿æ€§æ˜ å°„å¾®è°ƒ (ç¤ºæ„)
    prob_ticks = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]
    for prob in prob_ticks:
        x_pos = prob * 100
        ax.vlines(x_pos, y_prob, y_prob + 0.3, colors='darkred')
        # æ—‹è½¬æ¦‚ç‡æ ‡ç­¾ï¼Œé˜²æ­¢é‡å 
        ax.text(x_pos, y_prob - 0.5, f"{prob:.0%}", ha='center', fontsize=9, rotation=0)

    # 5. ä¿®é¥°ä¸ä¿å­˜
    ax.set_ylim(-4, 12)
    ax.set_xlim(-15, 110)  # æ‰©å¤§å·¦è¾¹è·ç•™ç»™æ ‡ç­¾
    ax.axis('off')

    plt.title('EV71 ç¥ç»æ¯’åŠ›é£é™©é¢„æµ‹åˆ—çº¿å›¾ (ç†åŒ–ç‰¹å¾æ¨¡å‹)', fontsize=16, pad=30)

    # å¢åŠ åº•éƒ¨æ³¨é‡Šè¯´æ˜
    plt.figtext(0.5, 0.05, "æ³¨ï¼š-3 è‡³ 3 ä»£è¡¨ä½ç‚¹ Z-scale å€¼çš„æ³¢åŠ¨èŒƒå›´ï¼›Points ä»£è¡¨è¯¥ç‰¹å¾å¯¹æ¯’åŠ›çš„è´¡çŒ®å¾—åˆ†ã€‚",
                ha="center", fontsize=10, style='italic', color='gray')

    plt.savefig('Nomogram_Risk_Prediction_Refined.pdf', bbox_inches='tight')
    plt.close()
    print("âœ¨ åˆ—çº¿å›¾å·²ç”Ÿæˆã€‚")

# ==========================================
#     æ‰§è¡Œé€»è¾‘
# ==========================================
if __name__ == "__main__":
    # # ==========================================
    # # 1. åˆå§‹åŒ–ä¸æ•°æ®å‡†å¤‡
    # # ==========================================
    # source_csv = './data/training_data.csv'
    #
    # # å¦‚æœéœ€è¦é‡æ–°æ¸…æ´—æ•°æ®ï¼Œå–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Š
    # """
    # if os.path.exists('Master_Dataset.csv'):
    #     os.remove('Master_Dataset.csv')
    #
    # if os.path.exists(source_csv):
    #     df_master = step1_prepare_master(source_csv)
    #     selected_vars = step2_generate_clinical_table(df_master)
    # else:
    #     print(f"âŒ æ‰¾ä¸åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {source_csv}")
    # """
    #
    # # ==========================================
    # # 2. æ ¸å¿ƒåˆ†ææµç¨‹ (å·²å¯¹é½)
    # # ==========================================
    #
    # # æ£®æ—å›¾ç”Ÿæˆ
    # generate_forest_plot(
    #     input_table='Clinical_Adaptive_Table1.csv',
    #     top_n_plot=15,
    #     output_pdf='Neurovirulence_Forest_Plot.pdf'
    # )
    #
    # # è®­ç»ƒå¹¶å¯¼å‡ºæ¨¡å‹ (åŒ…å« ROC æ›²çº¿)
    # train_and_freeze_model(
    #     input_table='Clinical_Adaptive_Table1.csv',
    #     master_csv='EV71_Master_Dataset.csv',
    #     top_n_model=5,
    #     model_output_name='Combined_Predictive_Model.pkl',
    #     roc_pdf='Prediction_ROC_Curve.pdf'
    # )
    #
    # # å…«æ¨¡å‹å¯¹æ¯”å¹¶ä¿å­˜å† å†›å…¨å®¶æ¡¶
    # compare_eight_models_and_save_best(
    #     master_csv='Master_Dataset.csv',
    #     model_output_path='EV71_Best_Model_Package.pkl'
    # )
    #
    # # ç†åŒ–æ™¯è§‚çƒ­å›¾
    # plot_physicochemical_heatmap()
    #
    # # ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOOCV)
    # run_loocv_validation()
    #
    # # SHAP è§£é‡Šæ€§åˆ†æ
    # generate_shap_analysis()

    # åˆ—çº¿å›¾ç”Ÿæˆ (Nomogram)
    # generate_nomogram_data()

